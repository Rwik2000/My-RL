{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAHMB0Ze8fU0"
   },
   "outputs": [],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    # An element of torage basically stores a tuple of : state, next_state, action, reward, done\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      # Moving the pointer in circular manner.\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      # converting numpy to torch tensors\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 1e6 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fyH8N5z-o3o"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------\nSettings: TD3_AntBulletEnv-v0_0\n---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhC_5XJ__Orp"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------\nAverage Reward over the Evaluation Step: 9.804960\n---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_ouY4NH_Y0I"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: 471.0640800974418\n",
      "Total Timesteps: 2000 Episode Num: 2 Reward: 486.84410911087394\n",
      "Total Timesteps: 3000 Episode Num: 3 Reward: 471.1498236485527\n",
      "Total Timesteps: 4000 Episode Num: 4 Reward: 535.6631870644239\n",
      "Total Timesteps: 5000 Episode Num: 5 Reward: 474.54808309684284\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 217.137549\n",
      "---------------------------------------\n",
      "Total Timesteps: 6000 Episode Num: 6 Reward: 456.64327200424435\n",
      "Total Timesteps: 7000 Episode Num: 7 Reward: 466.7676302842195\n",
      "Total Timesteps: 7455 Episode Num: 8 Reward: 209.1581687561236\n",
      "Total Timesteps: 7686 Episode Num: 9 Reward: 103.38002780541575\n",
      "Total Timesteps: 8225 Episode Num: 10 Reward: 254.9223068664358\n",
      "Total Timesteps: 9225 Episode Num: 11 Reward: 480.4569344780614\n",
      "Total Timesteps: 10225 Episode Num: 12 Reward: 486.03187317208875\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 252.055659\n",
      "---------------------------------------\n",
      "Total Timesteps: 11225 Episode Num: 13 Reward: 293.9913572989483\n",
      "Total Timesteps: 12225 Episode Num: 14 Reward: 293.51177114072345\n",
      "Total Timesteps: 13225 Episode Num: 15 Reward: 91.53520582885699\n",
      "Total Timesteps: 14225 Episode Num: 16 Reward: 207.93046692528006\n",
      "Total Timesteps: 15225 Episode Num: 17 Reward: 92.9252610404772\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 221.991645\n",
      "---------------------------------------\n",
      "Total Timesteps: 16225 Episode Num: 18 Reward: 292.7446899449893\n",
      "Total Timesteps: 17225 Episode Num: 19 Reward: 193.7756128466495\n",
      "Total Timesteps: 18225 Episode Num: 20 Reward: 291.8812099194058\n",
      "Total Timesteps: 19225 Episode Num: 21 Reward: 289.71006881430026\n",
      "Total Timesteps: 20225 Episode Num: 22 Reward: 294.1258990798728\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 224.550314\n",
      "---------------------------------------\n",
      "Total Timesteps: 21225 Episode Num: 23 Reward: 294.473229678131\n",
      "Total Timesteps: 22225 Episode Num: 24 Reward: 194.33546568050554\n",
      "Total Timesteps: 23225 Episode Num: 25 Reward: 291.161467029843\n",
      "Total Timesteps: 24225 Episode Num: 26 Reward: 292.98878627199525\n",
      "Total Timesteps: 25225 Episode Num: 27 Reward: 170.89163692002253\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 151.587312\n",
      "---------------------------------------\n",
      "Total Timesteps: 26225 Episode Num: 28 Reward: 161.8186802708845\n",
      "Total Timesteps: 27225 Episode Num: 29 Reward: 288.75508291640733\n",
      "Total Timesteps: 28225 Episode Num: 30 Reward: 219.5932726187125\n",
      "Total Timesteps: 29225 Episode Num: 31 Reward: 298.288715936448\n",
      "Total Timesteps: 30225 Episode Num: 32 Reward: 96.99645603688512\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 96.466204\n",
      "---------------------------------------\n",
      "Total Timesteps: 31225 Episode Num: 33 Reward: 103.3194308143093\n",
      "Total Timesteps: 32225 Episode Num: 34 Reward: 93.17798824895742\n",
      "Total Timesteps: 33225 Episode Num: 35 Reward: 220.59914840106342\n",
      "Total Timesteps: 33396 Episode Num: 36 Reward: 13.862178512187286\n",
      "Total Timesteps: 33465 Episode Num: 37 Reward: 11.201703364156657\n",
      "Total Timesteps: 34465 Episode Num: 38 Reward: 193.16190691595423\n",
      "Total Timesteps: 35465 Episode Num: 39 Reward: 287.547577050074\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 147.820286\n",
      "---------------------------------------\n",
      "Total Timesteps: 36465 Episode Num: 40 Reward: 190.07009302010346\n",
      "Total Timesteps: 37465 Episode Num: 41 Reward: 95.25398630544055\n",
      "Total Timesteps: 38465 Episode Num: 42 Reward: 230.66173163479374\n",
      "Total Timesteps: 38547 Episode Num: 43 Reward: -0.6142130634408254\n",
      "Total Timesteps: 38620 Episode Num: 44 Reward: 1.9474826927518238\n",
      "Total Timesteps: 39620 Episode Num: 45 Reward: 171.19509526623267\n",
      "Total Timesteps: 40620 Episode Num: 46 Reward: 173.10114790080812\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 151.497939\n",
      "---------------------------------------\n",
      "Total Timesteps: 41620 Episode Num: 47 Reward: 186.3216836478531\n",
      "Total Timesteps: 42620 Episode Num: 48 Reward: 82.34967244245546\n",
      "Total Timesteps: 43620 Episode Num: 49 Reward: 194.220615841055\n",
      "Total Timesteps: 44620 Episode Num: 50 Reward: 190.55276351453884\n",
      "Total Timesteps: 45620 Episode Num: 51 Reward: 214.69821342081636\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 187.484569\n",
      "---------------------------------------\n",
      "Total Timesteps: 46620 Episode Num: 52 Reward: 286.531693126849\n",
      "Total Timesteps: 46732 Episode Num: 53 Reward: 5.835364799443784\n",
      "Total Timesteps: 47732 Episode Num: 54 Reward: 172.2976924072809\n",
      "Total Timesteps: 47863 Episode Num: 55 Reward: 5.072782495045354\n",
      "Total Timesteps: 48863 Episode Num: 56 Reward: 247.31779178768224\n",
      "Total Timesteps: 49863 Episode Num: 57 Reward: 199.71660104390952\n",
      "Total Timesteps: 50025 Episode Num: 58 Reward: -21.874143966521924\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 91.341358\n",
      "---------------------------------------\n",
      "Total Timesteps: 51025 Episode Num: 59 Reward: 97.62896552183253\n",
      "Total Timesteps: 52025 Episode Num: 60 Reward: 199.83363792032273\n",
      "Total Timesteps: 53025 Episode Num: 61 Reward: 196.94575759272087\n",
      "Total Timesteps: 54025 Episode Num: 62 Reward: 71.9259059631242\n",
      "Total Timesteps: 55025 Episode Num: 63 Reward: 442.537934562622\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 208.802392\n",
      "---------------------------------------\n",
      "Total Timesteps: 56025 Episode Num: 64 Reward: 198.96928393682384\n",
      "Total Timesteps: 57025 Episode Num: 65 Reward: 107.9220809255586\n",
      "Total Timesteps: 58025 Episode Num: 66 Reward: 176.22501040520186\n",
      "Total Timesteps: 58114 Episode Num: 67 Reward: -35.126422469268626\n",
      "Total Timesteps: 59114 Episode Num: 68 Reward: 230.00630447889867\n",
      "Total Timesteps: 60114 Episode Num: 69 Reward: 150.32129661946698\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 257.458405\n",
      "---------------------------------------\n",
      "Total Timesteps: 61114 Episode Num: 70 Reward: 217.141035924485\n",
      "Total Timesteps: 62114 Episode Num: 71 Reward: 86.18965911289293\n",
      "Total Timesteps: 63114 Episode Num: 72 Reward: 215.02055237391855\n",
      "Total Timesteps: 64114 Episode Num: 73 Reward: 300.79108204560373\n",
      "Total Timesteps: 65114 Episode Num: 74 Reward: 466.5259098595353\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 381.693531\n",
      "---------------------------------------\n",
      "Total Timesteps: 66114 Episode Num: 75 Reward: 326.4344980439168\n",
      "Total Timesteps: 67114 Episode Num: 76 Reward: 105.27105730249713\n",
      "Total Timesteps: 68114 Episode Num: 77 Reward: 597.808600706831\n",
      "Total Timesteps: 68199 Episode Num: 78 Reward: -1.9362589484891033\n",
      "Total Timesteps: 68359 Episode Num: 79 Reward: 6.813331567648541\n",
      "Total Timesteps: 69359 Episode Num: 80 Reward: 220.69567156771197\n",
      "Total Timesteps: 70359 Episode Num: 81 Reward: 493.63613810039794\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 478.015647\n",
      "---------------------------------------\n",
      "Total Timesteps: 71359 Episode Num: 82 Reward: 336.14134462904985\n",
      "Total Timesteps: 72359 Episode Num: 83 Reward: 505.2843980853975\n",
      "Total Timesteps: 72379 Episode Num: 84 Reward: 0.7534213531081853\n",
      "Total Timesteps: 72399 Episode Num: 85 Reward: 2.039816199019966\n",
      "Total Timesteps: 72419 Episode Num: 86 Reward: 2.0693596259754488\n",
      "Total Timesteps: 72439 Episode Num: 87 Reward: 2.400262827231793\n",
      "Total Timesteps: 72459 Episode Num: 88 Reward: 1.660063054977384\n",
      "Total Timesteps: 72479 Episode Num: 89 Reward: 1.5447308063697158\n",
      "Total Timesteps: 72500 Episode Num: 90 Reward: 1.9587098947214436\n",
      "Total Timesteps: 72612 Episode Num: 91 Reward: 30.690620093559723\n",
      "Total Timesteps: 73571 Episode Num: 92 Reward: 335.53080463241423\n",
      "Total Timesteps: 73591 Episode Num: 93 Reward: -2.802693143581842\n",
      "Total Timesteps: 74591 Episode Num: 94 Reward: 369.00978427372223\n",
      "Total Timesteps: 75591 Episode Num: 95 Reward: 363.2305727668613\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 199.427634\n",
      "---------------------------------------\n",
      "Total Timesteps: 76591 Episode Num: 96 Reward: 200.20475097043519\n",
      "Total Timesteps: 77591 Episode Num: 97 Reward: 228.0138954872852\n",
      "Total Timesteps: 78591 Episode Num: 98 Reward: 306.2204668178544\n",
      "Total Timesteps: 78611 Episode Num: 99 Reward: 1.1235258126788765\n",
      "Total Timesteps: 78631 Episode Num: 100 Reward: 1.5266833494253702\n",
      "Total Timesteps: 79631 Episode Num: 101 Reward: 408.75580797403404\n",
      "Total Timesteps: 79933 Episode Num: 102 Reward: 115.57660329832291\n",
      "Total Timesteps: 80933 Episode Num: 103 Reward: 417.27823072158657\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 30.371698\n",
      "---------------------------------------\n",
      "Total Timesteps: 80976 Episode Num: 104 Reward: 5.636676676175745\n",
      "Total Timesteps: 81168 Episode Num: 105 Reward: 85.963595599801\n",
      "Total Timesteps: 81218 Episode Num: 106 Reward: 9.11072493520342\n",
      "Total Timesteps: 81270 Episode Num: 107 Reward: 11.448986517673099\n",
      "Total Timesteps: 81322 Episode Num: 108 Reward: 11.402953989581814\n",
      "Total Timesteps: 81389 Episode Num: 109 Reward: 22.874439129682052\n",
      "Total Timesteps: 81490 Episode Num: 110 Reward: 23.356178704803675\n",
      "Total Timesteps: 81824 Episode Num: 111 Reward: 136.8326729065479\n",
      "Total Timesteps: 82824 Episode Num: 112 Reward: 305.402546947185\n",
      "Total Timesteps: 83311 Episode Num: 113 Reward: 212.47594596395143\n",
      "Total Timesteps: 84311 Episode Num: 114 Reward: 332.28301173903503\n",
      "Total Timesteps: 85158 Episode Num: 115 Reward: 283.50105847150456\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 312.327016\n",
      "---------------------------------------\n",
      "Total Timesteps: 85928 Episode Num: 116 Reward: 255.12971194291265\n",
      "Total Timesteps: 86140 Episode Num: 117 Reward: 99.86612856322459\n",
      "Total Timesteps: 86330 Episode Num: 118 Reward: 66.37407290283875\n",
      "Total Timesteps: 87330 Episode Num: 119 Reward: 435.2840365469869\n",
      "Total Timesteps: 87556 Episode Num: 120 Reward: 92.94699633251865\n",
      "Total Timesteps: 88556 Episode Num: 121 Reward: 456.8514499767113\n",
      "Total Timesteps: 89556 Episode Num: 122 Reward: 532.7365139603619\n",
      "Total Timesteps: 89580 Episode Num: 123 Reward: 6.284764074686836\n",
      "Total Timesteps: 89601 Episode Num: 124 Reward: 3.301311015941978\n",
      "Total Timesteps: 89621 Episode Num: 125 Reward: 3.229872209823994\n",
      "Total Timesteps: 89641 Episode Num: 126 Reward: 3.194256191349039\n",
      "Total Timesteps: 89661 Episode Num: 127 Reward: 2.6605713213914908\n",
      "Total Timesteps: 89681 Episode Num: 128 Reward: 2.504416941194802\n",
      "Total Timesteps: 89701 Episode Num: 129 Reward: 2.517334867261276\n",
      "Total Timesteps: 89721 Episode Num: 130 Reward: 2.2254618234086583\n",
      "Total Timesteps: 89741 Episode Num: 131 Reward: 3.081726358852691\n",
      "Total Timesteps: 89761 Episode Num: 132 Reward: 2.251650824692705\n",
      "Total Timesteps: 89781 Episode Num: 133 Reward: 3.1656634116226283\n",
      "Total Timesteps: 89801 Episode Num: 134 Reward: 3.1345600886436475\n",
      "Total Timesteps: 89821 Episode Num: 135 Reward: 3.273700877114488\n",
      "Total Timesteps: 89841 Episode Num: 136 Reward: 3.0185566725814907\n",
      "Total Timesteps: 89861 Episode Num: 137 Reward: 2.9590319049265554\n",
      "Total Timesteps: 89881 Episode Num: 138 Reward: 2.7303225525877894\n",
      "Total Timesteps: 89901 Episode Num: 139 Reward: 2.989687184681572\n",
      "Total Timesteps: 89921 Episode Num: 140 Reward: 3.1323564803341\n",
      "Total Timesteps: 89941 Episode Num: 141 Reward: 3.1436859263796553\n",
      "Total Timesteps: 89961 Episode Num: 142 Reward: 2.6166085549782965\n",
      "Total Timesteps: 89981 Episode Num: 143 Reward: 3.1013158873140414\n",
      "Total Timesteps: 90001 Episode Num: 144 Reward: 2.560720145053968\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 3.223328\n",
      "---------------------------------------\n",
      "Total Timesteps: 90021 Episode Num: 145 Reward: 3.496914699846708\n",
      "Total Timesteps: 90041 Episode Num: 146 Reward: 3.047632300013489\n",
      "Total Timesteps: 90061 Episode Num: 147 Reward: 2.500075990321171\n",
      "Total Timesteps: 90081 Episode Num: 148 Reward: 3.1112861902104565\n",
      "Total Timesteps: 90101 Episode Num: 149 Reward: 2.6812389377279153\n",
      "Total Timesteps: 90121 Episode Num: 150 Reward: 2.9092735793101654\n",
      "Total Timesteps: 90141 Episode Num: 151 Reward: 2.7400076814612024\n",
      "Total Timesteps: 90161 Episode Num: 152 Reward: 2.7833341713844924\n",
      "Total Timesteps: 90181 Episode Num: 153 Reward: 2.765662258022508\n",
      "Total Timesteps: 90201 Episode Num: 154 Reward: 3.141718570353557\n",
      "Total Timesteps: 90221 Episode Num: 155 Reward: 1.986407385108505\n",
      "Total Timesteps: 90241 Episode Num: 156 Reward: 2.9041524838769943\n",
      "Total Timesteps: 90261 Episode Num: 157 Reward: 2.6633428221271114\n",
      "Total Timesteps: 90281 Episode Num: 158 Reward: 2.802127380929732\n",
      "Total Timesteps: 90301 Episode Num: 159 Reward: 2.9429939648367864\n",
      "Total Timesteps: 90321 Episode Num: 160 Reward: 3.185187084678885\n",
      "Total Timesteps: 90341 Episode Num: 161 Reward: 2.524171828336332\n",
      "Total Timesteps: 90361 Episode Num: 162 Reward: 2.245102033554116\n",
      "Total Timesteps: 90708 Episode Num: 163 Reward: 175.78397507345605\n",
      "Total Timesteps: 91708 Episode Num: 164 Reward: 457.43379212871537\n",
      "Total Timesteps: 92708 Episode Num: 165 Reward: 562.3213106968822\n",
      "Total Timesteps: 93708 Episode Num: 166 Reward: 412.8503810307226\n",
      "Total Timesteps: 94708 Episode Num: 167 Reward: 293.32561094670876\n",
      "Total Timesteps: 95708 Episode Num: 168 Reward: 525.3420422096592\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 455.965858\n",
      "---------------------------------------\n",
      "Total Timesteps: 96708 Episode Num: 169 Reward: 411.5150016841983\n",
      "Total Timesteps: 97708 Episode Num: 170 Reward: 420.4650989576583\n",
      "Total Timesteps: 98708 Episode Num: 171 Reward: 494.4448450054208\n",
      "Total Timesteps: 99708 Episode Num: 172 Reward: 569.329069915481\n",
      "Total Timesteps: 100708 Episode Num: 173 Reward: 459.3254419904317\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 411.209645\n",
      "---------------------------------------\n",
      "Total Timesteps: 101708 Episode Num: 174 Reward: 316.32756280924684\n",
      "Total Timesteps: 102708 Episode Num: 175 Reward: 625.9254684688875\n",
      "Total Timesteps: 103708 Episode Num: 176 Reward: 638.9982452746848\n",
      "Total Timesteps: 104708 Episode Num: 177 Reward: 562.4719279873742\n",
      "Total Timesteps: 105708 Episode Num: 178 Reward: 416.6532658040936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 548.905360\n",
      "---------------------------------------\n",
      "Total Timesteps: 106708 Episode Num: 179 Reward: 467.8545336110312\n",
      "Total Timesteps: 107708 Episode Num: 180 Reward: 419.2385228080191\n",
      "Total Timesteps: 108708 Episode Num: 181 Reward: 545.8180051677316\n",
      "Total Timesteps: 109708 Episode Num: 182 Reward: 561.140138753354\n",
      "Total Timesteps: 110708 Episode Num: 183 Reward: 673.4611237028166\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 236.949742\n",
      "---------------------------------------\n",
      "Total Timesteps: 111708 Episode Num: 184 Reward: 324.8190889654961\n",
      "Total Timesteps: 112622 Episode Num: 185 Reward: 303.1044060378442\n",
      "Total Timesteps: 113622 Episode Num: 186 Reward: 309.8564878672374\n",
      "Total Timesteps: 114622 Episode Num: 187 Reward: 330.67025421503604\n",
      "Total Timesteps: 115622 Episode Num: 188 Reward: 605.883764283701\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 370.988351\n",
      "---------------------------------------\n",
      "Total Timesteps: 116622 Episode Num: 189 Reward: 299.1654330183263\n",
      "Total Timesteps: 117622 Episode Num: 190 Reward: 425.5843615195226\n",
      "Total Timesteps: 118622 Episode Num: 191 Reward: 410.22376040881\n",
      "Total Timesteps: 119622 Episode Num: 192 Reward: 396.36837937778154\n",
      "Total Timesteps: 120622 Episode Num: 193 Reward: 316.76098749381777\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 355.269387\n",
      "---------------------------------------\n",
      "Total Timesteps: 121622 Episode Num: 194 Reward: 495.5300497642202\n",
      "Total Timesteps: 122622 Episode Num: 195 Reward: 436.50876508476864\n",
      "Total Timesteps: 123622 Episode Num: 196 Reward: 408.26350168055586\n",
      "Total Timesteps: 124622 Episode Num: 197 Reward: 503.52164605974946\n",
      "Total Timesteps: 125622 Episode Num: 198 Reward: 517.2590531345253\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 385.176677\n",
      "---------------------------------------\n",
      "Total Timesteps: 126622 Episode Num: 199 Reward: 606.5848178125667\n",
      "Total Timesteps: 127622 Episode Num: 200 Reward: 536.9688673504631\n",
      "Total Timesteps: 127774 Episode Num: 201 Reward: 89.38261506410547\n",
      "Total Timesteps: 128774 Episode Num: 202 Reward: 371.4549356024945\n",
      "Total Timesteps: 129774 Episode Num: 203 Reward: 613.0868367870604\n",
      "Total Timesteps: 130774 Episode Num: 204 Reward: 420.09775786514444\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 434.712330\n",
      "---------------------------------------\n",
      "Total Timesteps: 131774 Episode Num: 205 Reward: 356.22300411319634\n",
      "Total Timesteps: 132774 Episode Num: 206 Reward: 515.8278262248264\n",
      "Total Timesteps: 133774 Episode Num: 207 Reward: 485.6432291063209\n",
      "Total Timesteps: 134774 Episode Num: 208 Reward: 288.4573584395294\n",
      "Total Timesteps: 135774 Episode Num: 209 Reward: 638.624239926578\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 478.570399\n",
      "---------------------------------------\n",
      "Total Timesteps: 136774 Episode Num: 210 Reward: 778.9401564100423\n",
      "Total Timesteps: 137774 Episode Num: 211 Reward: 497.06105319549266\n",
      "Total Timesteps: 138774 Episode Num: 212 Reward: 633.0490807164203\n",
      "Total Timesteps: 139774 Episode Num: 213 Reward: 283.47990607085876\n",
      "Total Timesteps: 140774 Episode Num: 214 Reward: 474.23620134616095\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 354.834561\n",
      "---------------------------------------\n",
      "Total Timesteps: 141774 Episode Num: 215 Reward: 275.91810344934544\n",
      "Total Timesteps: 142774 Episode Num: 216 Reward: 646.598592878282\n",
      "Total Timesteps: 143774 Episode Num: 217 Reward: 338.1210594519124\n",
      "Total Timesteps: 144774 Episode Num: 218 Reward: 444.6140293677585\n",
      "Total Timesteps: 145774 Episode Num: 219 Reward: 229.50706081825172\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 347.907906\n",
      "---------------------------------------\n",
      "Total Timesteps: 146774 Episode Num: 220 Reward: 604.7981602253464\n",
      "Total Timesteps: 147774 Episode Num: 221 Reward: 300.08270396309814\n",
      "Total Timesteps: 148774 Episode Num: 222 Reward: 374.45946751158203\n",
      "Total Timesteps: 149774 Episode Num: 223 Reward: 434.19073046449233\n",
      "Total Timesteps: 150774 Episode Num: 224 Reward: 178.55017983861958\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 331.384919\n",
      "---------------------------------------\n",
      "Total Timesteps: 151774 Episode Num: 225 Reward: 340.28981062897776\n",
      "Total Timesteps: 152774 Episode Num: 226 Reward: 321.2111491090977\n",
      "Total Timesteps: 153774 Episode Num: 227 Reward: 524.9745325750695\n",
      "Total Timesteps: 154774 Episode Num: 228 Reward: 426.56906958398065\n",
      "Total Timesteps: 155774 Episode Num: 229 Reward: 685.997499981511\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 475.186563\n",
      "---------------------------------------\n",
      "Total Timesteps: 156774 Episode Num: 230 Reward: 458.02528714661736\n",
      "Total Timesteps: 157774 Episode Num: 231 Reward: 615.7922694020076\n",
      "Total Timesteps: 158774 Episode Num: 232 Reward: 534.2602087778965\n",
      "Total Timesteps: 159774 Episode Num: 233 Reward: 549.6232915431951\n",
      "Total Timesteps: 160774 Episode Num: 234 Reward: 339.2293957128748\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 504.931239\n",
      "---------------------------------------\n",
      "Total Timesteps: 161774 Episode Num: 235 Reward: 502.1640583922411\n",
      "Total Timesteps: 162774 Episode Num: 236 Reward: 635.6665806947344\n",
      "Total Timesteps: 163774 Episode Num: 237 Reward: 440.63506712921776\n",
      "Total Timesteps: 164774 Episode Num: 238 Reward: 550.5503939345153\n",
      "Total Timesteps: 165774 Episode Num: 239 Reward: 466.93923521747917\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 367.320463\n",
      "---------------------------------------\n",
      "Total Timesteps: 166774 Episode Num: 240 Reward: 310.657162873461\n",
      "Total Timesteps: 167774 Episode Num: 241 Reward: 245.6099345017599\n",
      "Total Timesteps: 168774 Episode Num: 242 Reward: 417.3301154637135\n",
      "Total Timesteps: 169774 Episode Num: 243 Reward: 439.5060757895908\n",
      "Total Timesteps: 170774 Episode Num: 244 Reward: 159.3125292170458\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 422.313141\n",
      "---------------------------------------\n",
      "Total Timesteps: 171774 Episode Num: 245 Reward: 449.25841040121566\n",
      "Total Timesteps: 172774 Episode Num: 246 Reward: 406.21222261426425\n",
      "Total Timesteps: 173774 Episode Num: 247 Reward: 448.1998911612773\n",
      "Total Timesteps: 174363 Episode Num: 248 Reward: 204.76097822346358\n",
      "Total Timesteps: 175363 Episode Num: 249 Reward: 320.2932589491915\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 383.739475\n",
      "---------------------------------------\n",
      "Total Timesteps: 176363 Episode Num: 250 Reward: 509.1488734969939\n",
      "Total Timesteps: 177363 Episode Num: 251 Reward: 329.0618327313434\n",
      "Total Timesteps: 178363 Episode Num: 252 Reward: 340.679773870709\n",
      "Total Timesteps: 179363 Episode Num: 253 Reward: 441.6454046718878\n",
      "Total Timesteps: 180363 Episode Num: 254 Reward: 530.9459777539155\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 366.439374\n",
      "---------------------------------------\n",
      "Total Timesteps: 181363 Episode Num: 255 Reward: 295.71814610700045\n",
      "Total Timesteps: 182363 Episode Num: 256 Reward: 312.30529728467565\n",
      "Total Timesteps: 183198 Episode Num: 257 Reward: 335.3401631979632\n",
      "Total Timesteps: 184198 Episode Num: 258 Reward: 423.53845269041574\n",
      "Total Timesteps: 185198 Episode Num: 259 Reward: 422.2656208709414\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 303.274124\n",
      "---------------------------------------\n",
      "Total Timesteps: 186198 Episode Num: 260 Reward: 295.6695968663901\n",
      "Total Timesteps: 187198 Episode Num: 261 Reward: 458.4824659214677\n",
      "Total Timesteps: 188198 Episode Num: 262 Reward: 448.2583181873415\n",
      "Total Timesteps: 189198 Episode Num: 263 Reward: 445.7080453709493\n",
      "Total Timesteps: 190198 Episode Num: 264 Reward: 485.5375711164964\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 461.329805\n",
      "---------------------------------------\n",
      "Total Timesteps: 191198 Episode Num: 265 Reward: 664.7890093979421\n",
      "Total Timesteps: 192198 Episode Num: 266 Reward: 642.4326324138208\n",
      "Total Timesteps: 193198 Episode Num: 267 Reward: 424.31560517023786\n",
      "Total Timesteps: 194198 Episode Num: 268 Reward: 432.54371352587594\n",
      "Total Timesteps: 195198 Episode Num: 269 Reward: 192.19129454082525\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 485.781368\n",
      "---------------------------------------\n",
      "Total Timesteps: 196198 Episode Num: 270 Reward: 491.70190308000423\n",
      "Total Timesteps: 197198 Episode Num: 271 Reward: 392.7927133085327\n",
      "Total Timesteps: 198198 Episode Num: 272 Reward: 319.6999290437779\n",
      "Total Timesteps: 199198 Episode Num: 273 Reward: 695.9325543149768\n",
      "Total Timesteps: 200198 Episode Num: 274 Reward: 506.0440987801815\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 589.024688\n",
      "---------------------------------------\n",
      "Total Timesteps: 201198 Episode Num: 275 Reward: 623.5172139933688\n",
      "Total Timesteps: 202198 Episode Num: 276 Reward: 364.84669204206506\n",
      "Total Timesteps: 203198 Episode Num: 277 Reward: 286.9481236337796\n",
      "Total Timesteps: 204198 Episode Num: 278 Reward: 703.9394333075385\n",
      "Total Timesteps: 205198 Episode Num: 279 Reward: 393.38645503265565\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 159.063756\n",
      "---------------------------------------\n",
      "Total Timesteps: 206198 Episode Num: 280 Reward: 309.3929151786882\n",
      "Total Timesteps: 206807 Episode Num: 281 Reward: 325.03248256644235\n",
      "Total Timesteps: 207807 Episode Num: 282 Reward: 346.76580521744404\n",
      "Total Timesteps: 208807 Episode Num: 283 Reward: 127.88185522332132\n",
      "Total Timesteps: 209807 Episode Num: 284 Reward: 265.83946935432994\n",
      "Total Timesteps: 210807 Episode Num: 285 Reward: 233.95139648729574\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 423.835850\n",
      "---------------------------------------\n",
      "Total Timesteps: 211203 Episode Num: 286 Reward: 164.47918896509833\n",
      "Total Timesteps: 212203 Episode Num: 287 Reward: 717.5027690047664\n",
      "Total Timesteps: 212370 Episode Num: 288 Reward: 82.19855099683058\n",
      "Total Timesteps: 213370 Episode Num: 289 Reward: 460.44924235712625\n",
      "Total Timesteps: 214370 Episode Num: 290 Reward: 407.7365142909628\n",
      "Total Timesteps: 215370 Episode Num: 291 Reward: 286.21454193868175\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 358.717780\n",
      "---------------------------------------\n",
      "Total Timesteps: 216370 Episode Num: 292 Reward: 389.3516695470759\n",
      "Total Timesteps: 216605 Episode Num: 293 Reward: 115.63763051008229\n",
      "Total Timesteps: 216728 Episode Num: 294 Reward: 51.20932785566826\n",
      "Total Timesteps: 217728 Episode Num: 295 Reward: 252.94938676597673\n",
      "Total Timesteps: 217846 Episode Num: 296 Reward: 55.636815319507605\n",
      "Total Timesteps: 218846 Episode Num: 297 Reward: 264.4569122428697\n",
      "Total Timesteps: 218941 Episode Num: 298 Reward: 53.07576800061211\n",
      "Total Timesteps: 219072 Episode Num: 299 Reward: 56.7769846429655\n",
      "Total Timesteps: 220072 Episode Num: 300 Reward: 702.5117680748756\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 439.108925\n",
      "---------------------------------------\n",
      "Total Timesteps: 221072 Episode Num: 301 Reward: 637.0910972590898\n",
      "Total Timesteps: 222072 Episode Num: 302 Reward: 704.7481390792976\n",
      "Total Timesteps: 223072 Episode Num: 303 Reward: 568.8937925278304\n",
      "Total Timesteps: 224072 Episode Num: 304 Reward: 480.8741786300321\n",
      "Total Timesteps: 224209 Episode Num: 305 Reward: 92.52965971356093\n",
      "Total Timesteps: 225209 Episode Num: 306 Reward: 451.63922635904987\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 406.477183\n",
      "---------------------------------------\n",
      "Total Timesteps: 226209 Episode Num: 307 Reward: 349.5329464131194\n",
      "Total Timesteps: 226330 Episode Num: 308 Reward: 45.245064029656135\n",
      "Total Timesteps: 227330 Episode Num: 309 Reward: 167.04837623456515\n",
      "Total Timesteps: 228330 Episode Num: 310 Reward: 169.19789328048202\n",
      "Total Timesteps: 229330 Episode Num: 311 Reward: 230.33188428991693\n",
      "Total Timesteps: 230330 Episode Num: 312 Reward: 273.18966505285204\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 448.320010\n",
      "---------------------------------------\n",
      "Total Timesteps: 231330 Episode Num: 313 Reward: 413.8302823609477\n",
      "Total Timesteps: 232330 Episode Num: 314 Reward: 422.20069896325896\n",
      "Total Timesteps: 233330 Episode Num: 315 Reward: 402.7050618069929\n",
      "Total Timesteps: 234330 Episode Num: 316 Reward: 388.04425056082584\n",
      "Total Timesteps: 235330 Episode Num: 317 Reward: 336.9959717492215\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 366.580591\n",
      "---------------------------------------\n",
      "Total Timesteps: 236330 Episode Num: 318 Reward: 403.3675608294583\n",
      "Total Timesteps: 237330 Episode Num: 319 Reward: 437.67359748612546\n",
      "Total Timesteps: 238330 Episode Num: 320 Reward: 522.356841518825\n",
      "Total Timesteps: 239330 Episode Num: 321 Reward: 521.3912569780749\n",
      "Total Timesteps: 240330 Episode Num: 322 Reward: 609.0561175009594\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 392.256862\n",
      "---------------------------------------\n",
      "Total Timesteps: 241330 Episode Num: 323 Reward: 532.3087454144488\n",
      "Total Timesteps: 242330 Episode Num: 324 Reward: 612.9365714131534\n",
      "Total Timesteps: 243330 Episode Num: 325 Reward: 496.37579147034484\n",
      "Total Timesteps: 244330 Episode Num: 326 Reward: 616.7590397217156\n",
      "Total Timesteps: 245330 Episode Num: 327 Reward: 448.53188761063967\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 496.382140\n",
      "---------------------------------------\n",
      "Total Timesteps: 246330 Episode Num: 328 Reward: 125.86627250299146\n",
      "Total Timesteps: 247330 Episode Num: 329 Reward: 372.26867777654627\n",
      "Total Timesteps: 248330 Episode Num: 330 Reward: 415.079985928497\n",
      "Total Timesteps: 249330 Episode Num: 331 Reward: 427.00555877918254\n",
      "Total Timesteps: 250330 Episode Num: 332 Reward: 599.4878133111932\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 545.216243\n",
      "---------------------------------------\n",
      "Total Timesteps: 251330 Episode Num: 333 Reward: 596.73950661718\n",
      "Total Timesteps: 252330 Episode Num: 334 Reward: 611.8075820600997\n",
      "Total Timesteps: 253330 Episode Num: 335 Reward: 664.6964498975319\n",
      "Total Timesteps: 254330 Episode Num: 336 Reward: 583.3814323368791\n",
      "Total Timesteps: 255330 Episode Num: 337 Reward: 470.4997716121522\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 439.935498\n",
      "---------------------------------------\n",
      "Total Timesteps: 256330 Episode Num: 338 Reward: 474.9871174254019\n",
      "Total Timesteps: 257330 Episode Num: 339 Reward: 508.6760457805101\n",
      "Total Timesteps: 258330 Episode Num: 340 Reward: 586.7647809367662\n",
      "Total Timesteps: 259330 Episode Num: 341 Reward: 695.8423703029526\n",
      "Total Timesteps: 260330 Episode Num: 342 Reward: 563.8769797740728\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 535.761141\n",
      "---------------------------------------\n",
      "Total Timesteps: 261330 Episode Num: 343 Reward: 520.4416004487557\n",
      "Total Timesteps: 262330 Episode Num: 344 Reward: 768.2276496660213\n",
      "Total Timesteps: 263330 Episode Num: 345 Reward: 629.3687760649443\n",
      "Total Timesteps: 264330 Episode Num: 346 Reward: 627.6698213464875\n",
      "Total Timesteps: 265330 Episode Num: 347 Reward: 617.6077227397199\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 429.516618\n",
      "---------------------------------------\n",
      "Total Timesteps: 266330 Episode Num: 348 Reward: 203.94984418364737\n",
      "Total Timesteps: 267330 Episode Num: 349 Reward: 323.0684126956529\n",
      "Total Timesteps: 268330 Episode Num: 350 Reward: 401.696701827193\n",
      "Total Timesteps: 269330 Episode Num: 351 Reward: 544.304311720171\n",
      "Total Timesteps: 270330 Episode Num: 352 Reward: 485.4270728815532\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 513.862491\n",
      "---------------------------------------\n",
      "Total Timesteps: 271330 Episode Num: 353 Reward: 486.30247151725194\n",
      "Total Timesteps: 272330 Episode Num: 354 Reward: 514.6925299401721\n",
      "Total Timesteps: 273330 Episode Num: 355 Reward: 480.65542338291203\n",
      "Total Timesteps: 274330 Episode Num: 356 Reward: 661.4984601405231\n",
      "Total Timesteps: 275330 Episode Num: 357 Reward: 428.1994349875605\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 660.681028\n",
      "---------------------------------------\n",
      "Total Timesteps: 276330 Episode Num: 358 Reward: 782.4767868873927\n",
      "Total Timesteps: 277330 Episode Num: 359 Reward: 200.34148408020084\n",
      "Total Timesteps: 278330 Episode Num: 360 Reward: 628.6846657386516\n",
      "Total Timesteps: 279330 Episode Num: 361 Reward: 542.1332370652759\n",
      "Total Timesteps: 280330 Episode Num: 362 Reward: 577.0327216016398\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 632.377015\n",
      "---------------------------------------\n",
      "Total Timesteps: 281330 Episode Num: 363 Reward: 511.57800037481815\n",
      "Total Timesteps: 281539 Episode Num: 364 Reward: 116.8035827711337\n",
      "Total Timesteps: 282539 Episode Num: 365 Reward: 321.36236078490737\n",
      "Total Timesteps: 283539 Episode Num: 366 Reward: 414.0664690370046\n",
      "Total Timesteps: 284539 Episode Num: 367 Reward: 692.8108538800124\n",
      "Total Timesteps: 285539 Episode Num: 368 Reward: 445.07657910953617\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 230.012559\n",
      "---------------------------------------\n",
      "Total Timesteps: 285559 Episode Num: 369 Reward: 5.72729343528766\n",
      "Total Timesteps: 285580 Episode Num: 370 Reward: 2.890348921673152\n",
      "Total Timesteps: 285600 Episode Num: 371 Reward: 5.237282335896634\n",
      "Total Timesteps: 286600 Episode Num: 372 Reward: 587.211903453898\n",
      "Total Timesteps: 287600 Episode Num: 373 Reward: 386.31134888500384\n",
      "Total Timesteps: 288600 Episode Num: 374 Reward: 560.0734570522746\n",
      "Total Timesteps: 289477 Episode Num: 375 Reward: 338.3947285722158\n",
      "Total Timesteps: 289497 Episode Num: 376 Reward: 4.0121810804314615\n",
      "Total Timesteps: 290497 Episode Num: 377 Reward: 475.12914859015376\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 354.998450\n",
      "---------------------------------------\n",
      "Total Timesteps: 291497 Episode Num: 378 Reward: 326.15921946870867\n",
      "Total Timesteps: 292497 Episode Num: 379 Reward: 504.2035519819113\n",
      "Total Timesteps: 293497 Episode Num: 380 Reward: 626.9671407077648\n",
      "Total Timesteps: 294497 Episode Num: 381 Reward: 471.9411499023325\n",
      "Total Timesteps: 295497 Episode Num: 382 Reward: 299.3382568388001\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 305.747893\n",
      "---------------------------------------\n",
      "Total Timesteps: 296497 Episode Num: 383 Reward: 376.0694985726073\n",
      "Total Timesteps: 297497 Episode Num: 384 Reward: 692.0919435406853\n",
      "Total Timesteps: 298497 Episode Num: 385 Reward: 487.821874836724\n",
      "Total Timesteps: 299497 Episode Num: 386 Reward: 362.13862691932025\n",
      "Total Timesteps: 300497 Episode Num: 387 Reward: 130.14295285201015\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 509.205585\n",
      "---------------------------------------\n",
      "Total Timesteps: 301497 Episode Num: 388 Reward: 531.2369648121434\n",
      "Total Timesteps: 302497 Episode Num: 389 Reward: 428.49656094294323\n",
      "Total Timesteps: 303497 Episode Num: 390 Reward: 558.9110268011157\n",
      "Total Timesteps: 304497 Episode Num: 391 Reward: 433.0952605895366\n",
      "Total Timesteps: 305497 Episode Num: 392 Reward: 363.028641911119\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 144.048661\n",
      "---------------------------------------\n",
      "Total Timesteps: 306497 Episode Num: 393 Reward: 383.71819845733893\n",
      "Total Timesteps: 307497 Episode Num: 394 Reward: 208.81333715755227\n",
      "Total Timesteps: 308497 Episode Num: 395 Reward: 442.7803808884961\n",
      "Total Timesteps: 309497 Episode Num: 396 Reward: 172.158882888098\n",
      "Total Timesteps: 310497 Episode Num: 397 Reward: 444.18706082937524\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 498.222667\n",
      "---------------------------------------\n",
      "Total Timesteps: 311497 Episode Num: 398 Reward: 621.8293610884677\n",
      "Total Timesteps: 312497 Episode Num: 399 Reward: 539.0310502440278\n",
      "Total Timesteps: 312613 Episode Num: 400 Reward: 70.06779661529197\n",
      "Total Timesteps: 313613 Episode Num: 401 Reward: 201.5973695891328\n",
      "Total Timesteps: 314613 Episode Num: 402 Reward: 361.1092450806285\n",
      "Total Timesteps: 315613 Episode Num: 403 Reward: 447.7187874136067\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 315.070009\n",
      "---------------------------------------\n",
      "Total Timesteps: 316613 Episode Num: 404 Reward: 320.5506567748046\n",
      "Total Timesteps: 317613 Episode Num: 405 Reward: 326.5412842575321\n",
      "Total Timesteps: 318613 Episode Num: 406 Reward: 185.93014296542853\n",
      "Total Timesteps: 319613 Episode Num: 407 Reward: 454.0549064169953\n",
      "Total Timesteps: 320613 Episode Num: 408 Reward: 129.6789071215872\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 129.412700\n",
      "---------------------------------------\n",
      "Total Timesteps: 321613 Episode Num: 409 Reward: 283.0030242647397\n",
      "Total Timesteps: 322613 Episode Num: 410 Reward: 583.1714545622735\n",
      "Total Timesteps: 323613 Episode Num: 411 Reward: 342.3272804995465\n",
      "Total Timesteps: 324613 Episode Num: 412 Reward: 599.8385741631785\n",
      "Total Timesteps: 325613 Episode Num: 413 Reward: 554.1751578259183\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 304.781308\n",
      "---------------------------------------\n",
      "Total Timesteps: 326613 Episode Num: 414 Reward: 382.9025158753955\n",
      "Total Timesteps: 327613 Episode Num: 415 Reward: 555.7075533026853\n",
      "Total Timesteps: 328613 Episode Num: 416 Reward: 806.0943901964066\n",
      "Total Timesteps: 329613 Episode Num: 417 Reward: 424.84139164326234\n",
      "Total Timesteps: 330613 Episode Num: 418 Reward: 825.6760886209225\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 375.022011\n",
      "---------------------------------------\n",
      "Total Timesteps: 331613 Episode Num: 419 Reward: 352.02721026813305\n",
      "Total Timesteps: 332613 Episode Num: 420 Reward: 392.420344408852\n",
      "Total Timesteps: 333613 Episode Num: 421 Reward: 636.0381775927551\n",
      "Total Timesteps: 334613 Episode Num: 422 Reward: 453.849262988866\n",
      "Total Timesteps: 335613 Episode Num: 423 Reward: 348.3228206818099\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 413.342415\n",
      "---------------------------------------\n",
      "Total Timesteps: 336613 Episode Num: 424 Reward: 667.9480228103641\n",
      "Total Timesteps: 337613 Episode Num: 425 Reward: 425.2716656863483\n",
      "Total Timesteps: 338613 Episode Num: 426 Reward: 342.25671993344577\n",
      "Total Timesteps: 339613 Episode Num: 427 Reward: 537.4516734087618\n",
      "Total Timesteps: 340613 Episode Num: 428 Reward: 340.4486491027401\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 554.827198\n",
      "---------------------------------------\n",
      "Total Timesteps: 341017 Episode Num: 429 Reward: 220.0079090401865\n",
      "Total Timesteps: 342017 Episode Num: 430 Reward: 447.1231509832652\n",
      "Total Timesteps: 343017 Episode Num: 431 Reward: 659.097715419917\n",
      "Total Timesteps: 344017 Episode Num: 432 Reward: 532.0694408730427\n",
      "Total Timesteps: 345017 Episode Num: 433 Reward: 279.2885343631339\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 361.920402\n",
      "---------------------------------------\n",
      "Total Timesteps: 346017 Episode Num: 434 Reward: 597.3234408267763\n",
      "Total Timesteps: 347017 Episode Num: 435 Reward: 377.42368484986173\n",
      "Total Timesteps: 348017 Episode Num: 436 Reward: 386.35971673079774\n",
      "Total Timesteps: 349017 Episode Num: 437 Reward: 320.72734942637106\n",
      "Total Timesteps: 350017 Episode Num: 438 Reward: 494.9889395962987\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 587.898934\n",
      "---------------------------------------\n",
      "Total Timesteps: 351017 Episode Num: 439 Reward: 753.6890143134178\n",
      "Total Timesteps: 352017 Episode Num: 440 Reward: 676.271357796574\n",
      "Total Timesteps: 353017 Episode Num: 441 Reward: 689.2380356986984\n",
      "Total Timesteps: 354017 Episode Num: 442 Reward: 225.5439513116566\n",
      "Total Timesteps: 355017 Episode Num: 443 Reward: 456.5626539602175\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 658.234169\n",
      "---------------------------------------\n",
      "Total Timesteps: 356017 Episode Num: 444 Reward: 646.4119085888765\n",
      "Total Timesteps: 357017 Episode Num: 445 Reward: 481.0010420137201\n",
      "Total Timesteps: 358017 Episode Num: 446 Reward: 698.6132181184581\n",
      "Total Timesteps: 359017 Episode Num: 447 Reward: 687.7523702224839\n",
      "Total Timesteps: 360017 Episode Num: 448 Reward: 394.7894380844239\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 632.661063\n",
      "---------------------------------------\n",
      "Total Timesteps: 361017 Episode Num: 449 Reward: 696.1413618500557\n",
      "Total Timesteps: 362017 Episode Num: 450 Reward: 741.4651966274313\n",
      "Total Timesteps: 363017 Episode Num: 451 Reward: 750.3646826757063\n",
      "Total Timesteps: 364017 Episode Num: 452 Reward: 471.7593370735733\n",
      "Total Timesteps: 365017 Episode Num: 453 Reward: 421.6726834041465\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 504.176866\n",
      "---------------------------------------\n",
      "Total Timesteps: 366017 Episode Num: 454 Reward: 517.3071755128407\n",
      "Total Timesteps: 366217 Episode Num: 455 Reward: 87.98946357613558\n",
      "Total Timesteps: 367217 Episode Num: 456 Reward: 468.8972827229242\n",
      "Total Timesteps: 368217 Episode Num: 457 Reward: 471.5887945540308\n",
      "Total Timesteps: 369217 Episode Num: 458 Reward: 669.2779999738136\n",
      "Total Timesteps: 370217 Episode Num: 459 Reward: 766.2929353088504\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 544.026482\n",
      "---------------------------------------\n",
      "Total Timesteps: 371217 Episode Num: 460 Reward: 637.4144452288771\n",
      "Total Timesteps: 372217 Episode Num: 461 Reward: 486.04782358182854\n",
      "Total Timesteps: 373217 Episode Num: 462 Reward: 659.4193993156128\n",
      "Total Timesteps: 374217 Episode Num: 463 Reward: 568.4989301029991\n",
      "Total Timesteps: 375217 Episode Num: 464 Reward: 557.0247166620816\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 465.928845\n",
      "---------------------------------------\n",
      "Total Timesteps: 376217 Episode Num: 465 Reward: 508.88166285758655\n",
      "Total Timesteps: 377217 Episode Num: 466 Reward: 618.8880049781665\n",
      "Total Timesteps: 378217 Episode Num: 467 Reward: 561.9341890527987\n",
      "Total Timesteps: 379217 Episode Num: 468 Reward: 728.3661804521302\n",
      "Total Timesteps: 380217 Episode Num: 469 Reward: 618.3454486551216\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 602.289453\n",
      "---------------------------------------\n",
      "Total Timesteps: 381217 Episode Num: 470 Reward: 639.7421409951376\n",
      "Total Timesteps: 382217 Episode Num: 471 Reward: 340.49414190512016\n",
      "Total Timesteps: 383217 Episode Num: 472 Reward: 639.0971794217232\n",
      "Total Timesteps: 384217 Episode Num: 473 Reward: 735.6637891376802\n",
      "Total Timesteps: 385217 Episode Num: 474 Reward: 639.6777960956276\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 547.522097\n",
      "---------------------------------------\n",
      "Total Timesteps: 386217 Episode Num: 475 Reward: 553.5929861739417\n",
      "Total Timesteps: 387217 Episode Num: 476 Reward: 570.2752621258118\n",
      "Total Timesteps: 388217 Episode Num: 477 Reward: 636.5191521346491\n",
      "Total Timesteps: 389217 Episode Num: 478 Reward: 701.5475765854889\n",
      "Total Timesteps: 390217 Episode Num: 479 Reward: 460.8371338760531\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 526.643516\n",
      "---------------------------------------\n",
      "Total Timesteps: 391217 Episode Num: 480 Reward: 501.79300443934665\n",
      "Total Timesteps: 392217 Episode Num: 481 Reward: 484.80079411377676\n",
      "Total Timesteps: 393217 Episode Num: 482 Reward: 654.7088187369402\n",
      "Total Timesteps: 394217 Episode Num: 483 Reward: 688.7696427266925\n",
      "Total Timesteps: 395217 Episode Num: 484 Reward: 483.8850263650809\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 572.047133\n",
      "---------------------------------------\n",
      "Total Timesteps: 396217 Episode Num: 485 Reward: 538.3785329258346\n",
      "Total Timesteps: 397217 Episode Num: 486 Reward: 606.8651254948874\n",
      "Total Timesteps: 398217 Episode Num: 487 Reward: 694.2239054675666\n",
      "Total Timesteps: 399217 Episode Num: 488 Reward: 792.3649516732595\n",
      "Total Timesteps: 400217 Episode Num: 489 Reward: 484.1598762113263\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 677.521366\n",
      "---------------------------------------\n",
      "Total Timesteps: 401217 Episode Num: 490 Reward: 598.5281784601979\n",
      "Total Timesteps: 402217 Episode Num: 491 Reward: 740.3146532418126\n",
      "Total Timesteps: 403217 Episode Num: 492 Reward: 836.6864073922336\n",
      "Total Timesteps: 404217 Episode Num: 493 Reward: 564.0546038384056\n",
      "Total Timesteps: 405217 Episode Num: 494 Reward: 723.9318970963171\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 621.669146\n",
      "---------------------------------------\n",
      "Total Timesteps: 406217 Episode Num: 495 Reward: 527.5431430414988\n",
      "Total Timesteps: 407217 Episode Num: 496 Reward: 838.2032341921162\n",
      "Total Timesteps: 408217 Episode Num: 497 Reward: 776.1982700146394\n",
      "Total Timesteps: 409217 Episode Num: 498 Reward: 675.9287878259013\n",
      "Total Timesteps: 410217 Episode Num: 499 Reward: 823.2713936417506\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 636.379069\n",
      "---------------------------------------\n",
      "Total Timesteps: 411217 Episode Num: 500 Reward: 522.7414944142188\n",
      "Total Timesteps: 412217 Episode Num: 501 Reward: 752.0605706636779\n",
      "Total Timesteps: 413217 Episode Num: 502 Reward: 695.3599447750477\n",
      "Total Timesteps: 414217 Episode Num: 503 Reward: 632.4081415259298\n",
      "Total Timesteps: 415217 Episode Num: 504 Reward: 701.3153631045078\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 676.709504\n",
      "---------------------------------------\n",
      "Total Timesteps: 416217 Episode Num: 505 Reward: 411.759194437445\n",
      "Total Timesteps: 417217 Episode Num: 506 Reward: 680.1238319381391\n",
      "Total Timesteps: 418217 Episode Num: 507 Reward: 588.0115774639787\n",
      "Total Timesteps: 419217 Episode Num: 508 Reward: 614.3480072661717\n",
      "Total Timesteps: 420217 Episode Num: 509 Reward: 644.4695387893147\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 740.852139\n",
      "---------------------------------------\n",
      "Total Timesteps: 421217 Episode Num: 510 Reward: 656.8338853016057\n",
      "Total Timesteps: 422217 Episode Num: 511 Reward: 609.2567043149689\n",
      "Total Timesteps: 423217 Episode Num: 512 Reward: 552.0589843746449\n",
      "Total Timesteps: 424217 Episode Num: 513 Reward: 695.6423317867981\n",
      "Total Timesteps: 425217 Episode Num: 514 Reward: 584.334742905187\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 589.110771\n",
      "---------------------------------------\n",
      "Total Timesteps: 426217 Episode Num: 515 Reward: 700.0134611010727\n",
      "Total Timesteps: 427217 Episode Num: 516 Reward: 375.78542998848417\n",
      "Total Timesteps: 428217 Episode Num: 517 Reward: 719.7786931033338\n",
      "Total Timesteps: 429217 Episode Num: 518 Reward: 531.3758883955585\n",
      "Total Timesteps: 430217 Episode Num: 519 Reward: 648.95252335967\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 604.497937\n",
      "---------------------------------------\n",
      "Total Timesteps: 431217 Episode Num: 520 Reward: 761.238497016305\n",
      "Total Timesteps: 432217 Episode Num: 521 Reward: 507.8752519204886\n",
      "Total Timesteps: 433217 Episode Num: 522 Reward: 678.5227233875713\n",
      "Total Timesteps: 434217 Episode Num: 523 Reward: 721.3694087336095\n",
      "Total Timesteps: 435217 Episode Num: 524 Reward: 656.2672211410428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 689.326443\n",
      "---------------------------------------\n",
      "Total Timesteps: 436217 Episode Num: 525 Reward: 787.7315071730947\n",
      "Total Timesteps: 437217 Episode Num: 526 Reward: 654.8442717767917\n",
      "Total Timesteps: 438217 Episode Num: 527 Reward: 648.6866352830186\n",
      "Total Timesteps: 439217 Episode Num: 528 Reward: 638.0122396256966\n",
      "Total Timesteps: 440217 Episode Num: 529 Reward: 523.3053458774377\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 647.632266\n",
      "---------------------------------------\n",
      "Total Timesteps: 441217 Episode Num: 530 Reward: 649.7766371477542\n",
      "Total Timesteps: 442217 Episode Num: 531 Reward: 527.8174728769067\n",
      "Total Timesteps: 443217 Episode Num: 532 Reward: 807.6995425451493\n",
      "Total Timesteps: 444217 Episode Num: 533 Reward: 869.7887824241964\n",
      "Total Timesteps: 445217 Episode Num: 534 Reward: 815.7494971230069\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 754.435179\n",
      "---------------------------------------\n",
      "Total Timesteps: 446217 Episode Num: 535 Reward: 693.7310164696379\n",
      "Total Timesteps: 447217 Episode Num: 536 Reward: 430.28380133693594\n",
      "Total Timesteps: 448217 Episode Num: 537 Reward: 663.0662489999044\n",
      "Total Timesteps: 449217 Episode Num: 538 Reward: 683.9973008729334\n",
      "Total Timesteps: 450217 Episode Num: 539 Reward: 994.777217224015\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 846.575302\n",
      "---------------------------------------\n",
      "Total Timesteps: 451217 Episode Num: 540 Reward: 571.869150758046\n",
      "Total Timesteps: 452217 Episode Num: 541 Reward: 754.0383714855011\n",
      "Total Timesteps: 453217 Episode Num: 542 Reward: 962.7054602537776\n",
      "Total Timesteps: 454217 Episode Num: 543 Reward: 858.365298992597\n",
      "Total Timesteps: 455217 Episode Num: 544 Reward: 1006.8671485005332\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 889.065650\n",
      "---------------------------------------\n",
      "Total Timesteps: 456217 Episode Num: 545 Reward: 806.5365208014445\n",
      "Total Timesteps: 457217 Episode Num: 546 Reward: 1047.093577108851\n",
      "Total Timesteps: 458217 Episode Num: 547 Reward: 596.0060131033088\n",
      "Total Timesteps: 459217 Episode Num: 548 Reward: 806.7138086105813\n",
      "Total Timesteps: 460217 Episode Num: 549 Reward: 1025.4656804798158\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 960.336497\n",
      "---------------------------------------\n",
      "Total Timesteps: 461217 Episode Num: 550 Reward: 904.8315831578763\n",
      "Total Timesteps: 462217 Episode Num: 551 Reward: 950.1873084984477\n",
      "Total Timesteps: 463217 Episode Num: 552 Reward: 939.4069986132203\n",
      "Total Timesteps: 464217 Episode Num: 553 Reward: 922.7529728525528\n",
      "Total Timesteps: 465217 Episode Num: 554 Reward: 775.1588641057144\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 615.731503\n",
      "---------------------------------------\n",
      "Total Timesteps: 466217 Episode Num: 555 Reward: 482.81050500780924\n",
      "Total Timesteps: 467217 Episode Num: 556 Reward: 747.7973554393826\n",
      "Total Timesteps: 468217 Episode Num: 557 Reward: 705.0028985256837\n",
      "Total Timesteps: 469217 Episode Num: 558 Reward: 731.8419333317048\n",
      "Total Timesteps: 470217 Episode Num: 559 Reward: 564.7920093523051\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 949.704909\n",
      "---------------------------------------\n",
      "Total Timesteps: 471217 Episode Num: 560 Reward: 474.6001504218892\n",
      "Total Timesteps: 472217 Episode Num: 561 Reward: 851.7966704171128\n",
      "Total Timesteps: 473217 Episode Num: 562 Reward: 964.2329958973695\n",
      "Total Timesteps: 474217 Episode Num: 563 Reward: 1249.1293613958744\n",
      "Total Timesteps: 475217 Episode Num: 564 Reward: 1100.7825588440212\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 943.466648\n",
      "---------------------------------------\n",
      "Total Timesteps: 476217 Episode Num: 565 Reward: 935.9447305428317\n",
      "Total Timesteps: 477217 Episode Num: 566 Reward: 1095.4643130462944\n",
      "Total Timesteps: 478217 Episode Num: 567 Reward: 997.524184616247\n",
      "Total Timesteps: 479217 Episode Num: 568 Reward: 883.2770449052958\n",
      "Total Timesteps: 480217 Episode Num: 569 Reward: 1114.2590556595505\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1314.610371\n",
      "---------------------------------------\n",
      "Total Timesteps: 481217 Episode Num: 570 Reward: 1323.6588405460027\n",
      "Total Timesteps: 482217 Episode Num: 571 Reward: 1189.0795364826208\n",
      "Total Timesteps: 483217 Episode Num: 572 Reward: 949.5063500004368\n",
      "Total Timesteps: 484217 Episode Num: 573 Reward: 1277.5907941277478\n",
      "Total Timesteps: 485217 Episode Num: 574 Reward: 1156.386322801414\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1056.342628\n",
      "---------------------------------------\n",
      "Total Timesteps: 486217 Episode Num: 575 Reward: 1216.4135048323556\n",
      "Total Timesteps: 487217 Episode Num: 576 Reward: 1373.4870622489343\n",
      "Total Timesteps: 488217 Episode Num: 577 Reward: 1307.7837179191588\n",
      "Total Timesteps: 489217 Episode Num: 578 Reward: 1123.8119654552052\n",
      "Total Timesteps: 490217 Episode Num: 579 Reward: 1253.6545051521864\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1385.455562\n",
      "---------------------------------------\n",
      "Total Timesteps: 491217 Episode Num: 580 Reward: 1323.5585204237907\n",
      "Total Timesteps: 492217 Episode Num: 581 Reward: 1379.0862762840125\n",
      "Total Timesteps: 493217 Episode Num: 582 Reward: 997.0144716213354\n",
      "Total Timesteps: 494217 Episode Num: 583 Reward: 1278.6969781292078\n",
      "Total Timesteps: 495217 Episode Num: 584 Reward: 1464.9416076081657\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1364.344058\n",
      "---------------------------------------\n",
      "Total Timesteps: 496217 Episode Num: 585 Reward: 1290.2021661141935\n",
      "Total Timesteps: 497217 Episode Num: 586 Reward: 1359.9183586761624\n",
      "Total Timesteps: 498217 Episode Num: 587 Reward: 1393.2192850184285\n",
      "Total Timesteps: 499217 Episode Num: 588 Reward: 1397.2642139628863\n",
      "Total Timesteps: 500217 Episode Num: 589 Reward: 1344.3984720347519\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1165.505063\n",
      "---------------------------------------\n",
      "Total Timesteps: 501217 Episode Num: 590 Reward: 1357.6665492815014\n",
      "Total Timesteps: 502217 Episode Num: 591 Reward: 1131.5063795673168\n",
      "Total Timesteps: 503217 Episode Num: 592 Reward: 1329.7042633358897\n",
      "Total Timesteps: 504217 Episode Num: 593 Reward: 1102.7587703823804\n",
      "Total Timesteps: 505217 Episode Num: 594 Reward: 1366.3488942553877\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1473.871434\n",
      "---------------------------------------\n",
      "Total Timesteps: 506217 Episode Num: 595 Reward: 1498.3761751961672\n",
      "Total Timesteps: 507217 Episode Num: 596 Reward: 1438.8955118185047\n",
      "Total Timesteps: 508217 Episode Num: 597 Reward: 1513.548339997558\n",
      "Total Timesteps: 509217 Episode Num: 598 Reward: 1441.4409148999403\n",
      "Total Timesteps: 510217 Episode Num: 599 Reward: 1436.2951783895885\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1500.075781\n",
      "---------------------------------------\n",
      "Total Timesteps: 511217 Episode Num: 600 Reward: 1493.873761603868\n",
      "Total Timesteps: 512217 Episode Num: 601 Reward: 1553.8243123437555\n",
      "Total Timesteps: 513217 Episode Num: 602 Reward: 1515.6748886726136\n",
      "Total Timesteps: 514217 Episode Num: 603 Reward: 1497.162039701245\n",
      "Total Timesteps: 515217 Episode Num: 604 Reward: 1365.1479523174912\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1708.408519\n",
      "---------------------------------------\n",
      "Total Timesteps: 516217 Episode Num: 605 Reward: 1545.1807241078668\n",
      "Total Timesteps: 517217 Episode Num: 606 Reward: 1727.0411988385513\n",
      "Total Timesteps: 518217 Episode Num: 607 Reward: 1405.0710258959975\n",
      "Total Timesteps: 519217 Episode Num: 608 Reward: 561.3562951388674\n",
      "Total Timesteps: 520217 Episode Num: 609 Reward: 1541.3604821276817\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1581.787563\n",
      "---------------------------------------\n",
      "Total Timesteps: 521217 Episode Num: 610 Reward: 1592.7662953134627\n",
      "Total Timesteps: 522217 Episode Num: 611 Reward: 1407.1853715871875\n",
      "Total Timesteps: 523217 Episode Num: 612 Reward: 1569.0757429190235\n",
      "Total Timesteps: 524217 Episode Num: 613 Reward: 1550.283072868388\n",
      "Total Timesteps: 525217 Episode Num: 614 Reward: 1428.2417732570257\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1616.002088\n",
      "---------------------------------------\n",
      "Total Timesteps: 526217 Episode Num: 615 Reward: 1598.428688036861\n",
      "Total Timesteps: 527217 Episode Num: 616 Reward: 1581.2300813695144\n",
      "Total Timesteps: 528217 Episode Num: 617 Reward: 1577.50721473842\n",
      "Total Timesteps: 529217 Episode Num: 618 Reward: 1570.9542701232326\n",
      "Total Timesteps: 530217 Episode Num: 619 Reward: 1627.4923706851398\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1646.479718\n",
      "---------------------------------------\n",
      "Total Timesteps: 531217 Episode Num: 620 Reward: 1585.2521693737347\n",
      "Total Timesteps: 532217 Episode Num: 621 Reward: 1543.5429854701101\n",
      "Total Timesteps: 533217 Episode Num: 622 Reward: 1673.5829848690064\n",
      "Total Timesteps: 534217 Episode Num: 623 Reward: 1690.191831825433\n",
      "Total Timesteps: 535217 Episode Num: 624 Reward: 1770.528520819093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1741.482634\n",
      "---------------------------------------\n",
      "Total Timesteps: 536217 Episode Num: 625 Reward: 1675.4721537974751\n",
      "Total Timesteps: 537217 Episode Num: 626 Reward: 1587.9909467964485\n",
      "Total Timesteps: 538217 Episode Num: 627 Reward: 1752.2400499338323\n",
      "Total Timesteps: 539217 Episode Num: 628 Reward: 1615.1123522231064\n",
      "Total Timesteps: 540217 Episode Num: 629 Reward: 1817.9400443269526\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1843.372484\n",
      "---------------------------------------\n",
      "Total Timesteps: 541217 Episode Num: 630 Reward: 1851.731989543381\n",
      "Total Timesteps: 542217 Episode Num: 631 Reward: 1864.6693969326589\n",
      "Total Timesteps: 543217 Episode Num: 632 Reward: 1862.3714579908412\n",
      "Total Timesteps: 544217 Episode Num: 633 Reward: 1797.082553494997\n",
      "Total Timesteps: 545217 Episode Num: 634 Reward: 1884.0312966594543\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1755.233310\n",
      "---------------------------------------\n",
      "Total Timesteps: 546217 Episode Num: 635 Reward: 1748.5896234921975\n",
      "Total Timesteps: 547217 Episode Num: 636 Reward: 1885.2967143251506\n",
      "Total Timesteps: 548217 Episode Num: 637 Reward: 1867.221826329446\n",
      "Total Timesteps: 549217 Episode Num: 638 Reward: 1924.4751703388315\n",
      "Total Timesteps: 550217 Episode Num: 639 Reward: 1928.9399250928223\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1922.393446\n",
      "---------------------------------------\n",
      "Total Timesteps: 551217 Episode Num: 640 Reward: 1905.9855820100006\n",
      "Total Timesteps: 552217 Episode Num: 641 Reward: 1906.2022671613388\n",
      "Total Timesteps: 553217 Episode Num: 642 Reward: 1881.5051698798482\n",
      "Total Timesteps: 554217 Episode Num: 643 Reward: 1884.767227617267\n",
      "Total Timesteps: 555217 Episode Num: 644 Reward: 1939.3663635405792\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1918.199386\n",
      "---------------------------------------\n",
      "Total Timesteps: 556217 Episode Num: 645 Reward: 1904.183881289971\n",
      "Total Timesteps: 557217 Episode Num: 646 Reward: 1925.3164611831735\n",
      "Total Timesteps: 558217 Episode Num: 647 Reward: 1816.691970048308\n",
      "Total Timesteps: 559217 Episode Num: 648 Reward: 1751.715671093285\n",
      "Total Timesteps: 560217 Episode Num: 649 Reward: 1933.251572196964\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1864.679106\n",
      "---------------------------------------\n",
      "Total Timesteps: 561217 Episode Num: 650 Reward: 1883.600705934066\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-c8ab5ae51bbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total Timesteps: {} Episode Num: {} Reward: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# We evaluate the episode and we save the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-cd50a7acf798>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m       \u001b[1;31m# Step 5: From the next state s’, the Actor target plays the next action a’\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m       \u001b[1;31m# Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Rwik1234\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-cd50a7acf798>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_action\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Rwik1234\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Rwik1234\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Rwik1234\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW4d1YAMqif1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_AntBulletEnv-v0_0\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1857.034843\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"AntBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Ant.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}